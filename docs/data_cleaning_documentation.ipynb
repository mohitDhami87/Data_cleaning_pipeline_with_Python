{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa39e62f",
   "metadata": {},
   "source": [
    "# Load raw data\n",
    "\n",
    "Common formats: CSV, Excel, JSON, Parquet.\n",
    "\n",
    "Minimal steps and checks:\n",
    "- Detect and record input path/URL and file format.\n",
    "- Use streaming or `nrows` sampling for very large files.\n",
    "- Basic read examples (use appropriate library): `pandas.read_csv`, `pandas.read_excel`, `pandas.read_parquet`.\n",
    "- Use read parameters to improve safety and performance: `parse_dates`, `dtype`, `usecols`, `compression`, `nrows` for sampling.\n",
    "- After loading: inspect `df.head()`, `df.shape`, `df.info()` to validate column names and basic types.\n",
    "- Record provenance: input filename, read options, row counts (raw and after any immediate filtering)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b8c239",
   "metadata": {},
   "source": [
    "# Generate data profile (types, missingness, basic stats)\n",
    "\n",
    "Key profiling outputs to capture:\n",
    "- Data types: `df.dtypes` to find numeric, datetime, categorical, and object types.\n",
    "- Missingness: per-column missing counts (`df.isnull().sum()`) and percentages (`(df.isnull().mean()*100).round(2)`).\n",
    "- Numeric summaries: `df.describe(include='number')` for mean, std, min/max, quartiles.\n",
    "- Categorical summaries: frequency tables (`df[col].value_counts(dropna=False).head()`), unique counts (`df.nunique()`).\n",
    "- Correlations: correlation matrix for numeric features (`df.select_dtypes('number').corr()`).\n",
    "- Spot checks: sample unusual values (`df.sample(5)`) and check for inconsistent encodings or whitespace in string columns.\n",
    "- Optional full reports: use profiling tools like `ydata_profiling` (formerly pandas-profiling) to export an HTML report for sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd30cb",
   "metadata": {},
   "source": [
    "# Clean/convert column types\n",
    "\n",
    "Common conversions and validation steps:\n",
    "- Numeric coercion: convert numeric-like columns with `pd.to_numeric(..., errors='coerce')` and inspect resulting `NaN` counts before imputing or removing.\n",
    "- Datetime parsing: use `pd.to_datetime(..., errors='coerce')`; verify common formats and timezone awareness when needed.\n",
    "- Categorical conversion: convert low-cardinality strings to `category` dtype to save memory and clarify intent.\n",
    "- String normalization: trim whitespace, unify case, and replace empty strings with missing (`.str.strip().replace({'': None})`).\n",
    "- Missing-value handling: decide per-column strategy (drop, `fillna()` with a statistic or sentinel, or model-based imputation).\n",
    "- Validation: after conversions, re-check `df.dtypes`, `df.isnull().sum()`, and spot-check changed values with `value_counts()` or `head()`.\n",
    "- Persisting cleaned data: save to an efficient format such as Parquet (`df.to_parquet`) or CSV (`df.to_csv`) and record the cleaning steps applied."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
